{
    "summary": "The code utilizes SentenceTransformers for cosine similarity and generates titles, categories, and tags. It involves a class for embedding computations, indexing, searching through an embedding index, and preparing search results using pre-trained models.",
    "details": [
        {
            "comment": "This code imports necessary libraries and sets up a context manager for using the SentenceTransformers model. It allows for calculating cosine similarity between content chunks, generating titles based on content summaries, preserving file metadata timestamps, and possibly categorizing content with tags and categories.",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":0-35",
            "content": "from contextlib import contextmanager\nimport os\nfrom typing import Iterable, Optional, Union, overload, Literal\nfrom beartype import beartype\nimport weakref\nimport progressbar\nimport torch\n# use sys.path.append to insert dependencies\n# ask llm to give some potential tags & category for content chunks\n# calculate cosine similarity to existing content\n# ask the llm to use existing tag & category or create new ones.\n# check if the newly created tag & category exists and update\n# to create title:\n# summarize the content\n# generate title from summary\n# get the time:\n# usr rclone to preserve timestamp\n# get mtime from file metadata\n# set mirror path\nos.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\nimport sentence_transformers  # i recommend you to use cpu only.\nimport sentence_transformers.util\nfrom contextlib import contextmanager\n@contextmanager\ndef sentence_transformer_context(\n    model_name=\"distiluse-base-multilingual-cased-v1\", device=\"cpu\", **kwargs\n):\n    model = sentence_transformers.SentenceTransformer(\n        model_name, device=device, **kwargs"
        },
        {
            "comment": "The code initializes a SimilarityIndex object with a sentence transformer model and a list of candidates. It also creates an index, encodes multiple strings into embeddings, and encodes a single string into an embedding.",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":36-73",
            "content": "    )\n    try:\n        yield model\n    finally:\n        del model\n@beartype\nclass SimilarityIndex(object):\n    def __init__(\n        self,\n        model: sentence_transformers.SentenceTransformer,\n        candidates: Iterable[str] = [],\n    ):\n        self.init_properties(model)\n        self.insert_multiple_candidates(candidates)\n    def init_index(self):\n        self.word_index: list[str] = []\n        self.embedding_index: Optional[torch.Tensor] = None\n    def init_properties(self, model: sentence_transformers.SentenceTransformer):\n        self.init_index()\n        self._model_weakref_ = weakref.ref(model)\n    @property\n    def model(self):\n        return self._model_weakref_()\n    def encode_multiple(self, items: list[str]):\n        embed_list = []\n        for it in progressbar.progressbar(items):\n            embed_list.append(self.encode_single(it))\n        ret = torch.cat(embed_list, dim=0)\n        return ret\n    def encode_single(self, it: str):\n        embed: torch.Tensor = self.model.encode([it], convert_to_tensor=True)  # type: ignore"
        },
        {
            "comment": "The code defines a class that handles embedding computations and indexing for similarity calculations. It includes methods to encode single or multiple strings into embeddings, update the embedding index, insert single or multiple candidates into the index, and compute similarities based on the embedding index.",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":74-102",
            "content": "        return embed\n    def encode(self, it: Union[str, list[str]]):\n        if isinstance(it, str):\n            return self.encode_single(it)\n        else:\n            return self.encode_multiple(it)\n    def update_embedding_index(self, embed: torch.Tensor):\n        if self.embedding_index is None:\n            self.embedding_index = embed\n        else:\n            self.embedding_index = torch.cat([self.embedding_index, embed], dim=0)\n    def insert_single_candidate(self, candidate: str):\n        it = candidate.strip()\n        if it:\n            if it not in self.word_index:\n                self.word_index.append(it)\n                embed = self.encode_single(it)\n                self.update_embedding_index(embed)\n    def insert_multiple_candidates(self, candidates: Iterable[str]):\n        for it in progressbar.progressbar(candidates):\n            self.insert_single_candidate(it)\n    def compute_similarity(self, it: Union[str, list[str]]):\n        if self.embedding_index is None:\n            raise Exception(\"No embedding index yet. Cannot compute similarity.\")"
        },
        {
            "comment": "This code defines a class with methods for searching through an embedding index. The `search` method takes a query and optional parameters for the number of top results (top_k) and whether to return similarity scores (return_similarity). If no similarity is requested, it returns a list of matching results; if similarity is requested, it returns a dictionary with the index and similarity score for each result. The `get_similarity_info` and `prepare_search_results` methods help prepare the search results based on the input parameters.",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":103-141",
            "content": "        embed = self.encode(it)\n        similarity = sentence_transformers.util.cos_sim(embed, self.embedding_index)\n        similarity = torch.sum(similarity, dim=0)\n        return similarity\n    # first overload should agree with default keyword arguments\n    @overload\n    def search(\n        self,\n        query: Union[str, list[str]],\n        top_k: int = 10,\n        return_similarity: Literal[False] = False,\n    ) -> list[str]:\n        ...\n    @overload\n    def search(\n        self,\n        query: Union[str, list[str]],\n        top_k: int = 10,\n        return_similarity: Literal[True] = True,\n    ) -> dict[str, float]:\n        ...\n    def search(\n        self,\n        query,\n        top_k=10,\n        return_similarity=False,\n    ):\n        query_length, similarity_list, top_k_indices = self.get_similarity_info(\n            query, top_k\n        )\n        ret = self.prepare_search_results(\n            query_length, similarity_list, top_k_indices, return_similarity\n        )\n        return ret\n    def prepare_search_results("
        },
        {
            "comment": "1. Function returns a dictionary of word indices and their normalized similarity scores for top_k indices\n2. Function checks if index size is not zero and query length is not zero before returning True or False\n3. Computes the similarity and converts it to a list for further processing\n4. Gets top_k indices from the computed similarity\n5. Retrieves information related to similarity and top_k indices using get_similarity_info function",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":142-169",
            "content": "        self,\n        query_length: int,\n        similarity_list: list[float],\n        top_k_indices: list[int],\n        return_similarity: bool,\n    ):\n        if return_similarity:\n            return {\n                self.word_index[ind]: similarity_list[ind] / query_length\n                for ind in top_k_indices\n            }\n        else:\n            return [self.word_index[ind] for ind in top_k_indices]\n    def can_compute_similarity(self, query: Union[str, list[str]]):\n        query_length = 1 if isinstance(query, str) else len(query)\n        can_compute = self.index_size > 0 and query_length > 0\n        return can_compute, query_length\n    def compute_similarity_and_get_top_k_indices(\n        self, query: Union[str, list[str]], top_k: int = 10\n    ):\n        similarity = self.compute_similarity(query)\n        similarity_list = similarity.tolist()\n        top_k_indices = self.get_top_k_indices(similarity, top_k)\n        return similarity_list, top_k_indices\n    def get_similarity_info(self, query: Union[str, list[str]], top_k: int = 10):"
        },
        {
            "comment": "Code snippet is a part of a class that computes similarity between queries and text data. It checks if the query length is computable, then calculates similarity scores and top k indices using `compute_similarity_and_get_top_k_indices` method. If the size of index is less than or equal to top_k, it returns all indices, else it uses torch's topk function to get top_k indices.",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":170-206",
            "content": "        similarity_list = []\n        top_k_indices = []\n        can_compute, query_length = self.can_compute_similarity(query)\n        if can_compute:\n            (\n                similarity_list,\n                top_k_indices,\n            ) = self.compute_similarity_and_get_top_k_indices(query, top_k)\n        return query_length, similarity_list, top_k_indices\n    @property\n    def index_size(self):\n        return len(self.word_index)\n    def get_top_k_indices(self, similarity: torch.Tensor, top_k=10):\n        if self.index_size <= top_k:\n            top_k_indices = list(range(self.index_size))\n        else:\n            top_k_indices = torch.topk(similarity, top_k).indices.squeeze().tolist()\n        return top_k_indices\ndef test_main():\n    texts = [\"I love my pet dog and spend a lot of time with it.\", \"\u6211\u7231\u6211\u7684\u5ba0\u7269\u72d7\uff0c\u5e76\u4e14\u7ecf\u5e38\u548c\u5b83\u5728\u4e00\u8d77\u3002\"]\n    tags = [\n        \"dogs\",\n        \"cats\",\n        \"computer\",\n        \"tech\",\n        \"life\",\n        \"dress\",\n        \"cook\",\n        \"outfit\",\n        \"fixing\",\n        \"mechanics\",\n        \"car\","
        },
        {
            "comment": "Iterating through each text, using a pre-trained model for similarity search, and printing the results.",
            "location": "\"/media/root/Prima/hexo_blog_demo/blog_template/docs/src/similarity_utils.py\":207-219",
            "content": "        \"gasoline\",\n    ]\n    with sentence_transformer_context() as model:\n        sim_index = SimilarityIndex(model, candidates=tags)\n        for it in texts:\n            ret = sim_index.search(query=it)\n            print(it, \"->\", ret)\nif __name__ == \"__main__\":\n    test_main()"
        }
    ]
}