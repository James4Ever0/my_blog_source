{
    "100": {
        "file_id": 14,
        "content": "    content_without_metadata: str,\n    filename: str = \"<unknown>\",\n    word_limit: int = 30,\n    programming_language=\"markdown\",\n    char_limit: int = 1000,\n    line_limit: int = 15,\n    sample_size: Optional[int] = None,\n):\n    prompt_base = generate_summary_prompt_base(word_limit)\n    prompt_generator = generate_summary_prompt_generator(programming_language)\n    with llm_context(prompt_base) as model:\n        ret = process_content_and_return_result(\n            model,\n            prompt_generator,\n            filename,\n            content_without_metadata,\n            char_limit=char_limit,\n            line_limit=line_limit,\n            sample_size=sample_size,\n        )\n        return ret[\"summary\"]\n@beartype\ndef get_date_obj_by_file_ctime(filepath: str):\n    creation_timestamp = os.path.getctime(filepath)\n    date_obj = datetime.datetime.fromtimestamp(creation_timestamp)\n    return date_obj\n@beartype\ndef get_filename_without_extension(filepath: str):\n    base_filepath = os.path.basename(filepath)\n    filename_without_extension = re.sub(r\"\\.\\w+$\", \"\", base_filepath)",
        "type": "code",
        "location": "/remove_unwanted_notes.py:497-531"
    },
    "101": {
        "file_id": 14,
        "content": "This code contains functions for generating a summary from content, getting a file's creation date and filename without extension. The `generate_summary` function takes in content, generates a prompt based on word limit, and processes the content using a language model to return a summary. The `get_date_obj_by_file_ctime` function retrieves a file's creation timestamp and converts it into a datetime object. Lastly, the `get_filename_without_extension` function extracts the filename from a filepath and removes any file extension.",
        "type": "comment"
    },
    "102": {
        "file_id": 14,
        "content": "    return filename_without_extension\n@beartype\ndef get_date_obj_by_metadata(metadata: JSONDict):\n    for field in DATE_MITIGATION_FIELDS:\n        value = metadata.get(field, None)\n        date_obj = None\n        if isinstance(value, datetime.datetime):\n            date_obj = value\n        elif isinstance(value, str):\n            date_obj = parse_date_with_multiple_formats(CUSTOM_DATE_FORMATS, value)\n        if date_obj is not None:\n            return date_obj\n@beartype\ndef get_date_obj_by_filepath(filepath: str):\n    filename_without_extension = get_filename_without_extension(filepath)\n    date_obj = parse_date_with_multiple_formats(\n        CUSTOM_DATE_FORMATS, filename_without_extension\n    )\n    return date_obj\n@beartype\ndef generate_date_obj(filepath: str, metadata: JSONDict):\n    maybe_methods = (\n        lambda: get_date_obj_by_metadata(metadata),\n        lambda: get_date_obj_by_filepath(filepath),\n    )\n    fallback = lambda: get_date_obj_by_file_ctime(filepath)\n    return maybe_with_fallback(maybe_methods, fallback)",
        "type": "code",
        "location": "/remove_unwanted_notes.py:532-564"
    },
    "103": {
        "file_id": 14,
        "content": "The code defines three functions:\n1. get_date_obj_by_metadata() - extracts the date from metadata dictionary.\n2. get_date_obj_by_filepath() - extracts the date from the filename of the filepath provided.\n3. generate_date_obj() - tries to extract a date using either metadata or the filepath, and if it fails, falls back to extracting it from the file's creation time.",
        "type": "comment"
    },
    "104": {
        "file_id": 14,
        "content": "@beartype\ndef generate_date(filepath: str, metadata: JSONDict):\n    date_obj = generate_date_obj(filepath, metadata)\n    ret = render_datetime_as_hexo_format(date_obj)\n    return ret\n@beartype\ndef maybe_with_fallback(\n    maybe_methods: Iterable[Callable[[], Union[T, None]]], fallback: Callable[[], T]\n) -> T:\n    for it in maybe_methods:\n        obj = it()\n        if obj is not None:\n            return obj\n    return fallback()\n@beartype\ndef replace_double_quotes_as_single_quotes(content: str):\n    return content.replace('\"', \"'\")\n@beartype\ndef generate_content_metadata(\n    filepath: str,\n    content_without_metadata: str,\n    metadata: JSONDict,\n    tags_similarity_index: SimilarityIndex,\n    categories_similarity_index: SimilarityIndex,\n    tag_top_k: int = DEFAULT_TOP_K,\n    category_top_k: int = DEFAULT_TOP_K,\n    summary_word_limit: int = 30,\n    programming_language: str = \"markdown\",\n    char_limit: int = 1000,\n    line_limit: int = 15,\n    sample_size: Optional[int] = None,\n):\n    @beartype\n    def get_additional_metadata(",
        "type": "code",
        "location": "/remove_unwanted_notes.py:567-606"
    },
    "105": {
        "file_id": 14,
        "content": "This code defines several functions for generating content, metadata, and formatting dates. It uses type hints to specify the expected types of input parameters and returns the generated results using the Beartype decorator for type checking and error handling.",
        "type": "comment"
    },
    "106": {
        "file_id": 14,
        "content": "        missing_fields: Iterable[str], field_to_method: dict[str, Callable[[], str]]\n    ):\n        additional_metadata = {}\n        for field in missing_fields:\n            additional_metadata[field] = field_to_method[field]()\n        return purify_dict(additional_metadata)\n    @beartype\n    def generate_new_metadata(additional_metadata: JSONDict):\n        changed = False\n        new_metadata = metadata.copy()\n        if additional_metadata != {}:\n            changed = True\n            new_metadata.update(additional_metadata)\n        return new_metadata, changed\n    def build_field_generation_methods_with_summary():\n        summary = generate_summary(\n            content_without_metadata,\n            word_limit=summary_word_limit,\n            programming_language=programming_language,\n            char_limit=char_limit,\n            line_limit=line_limit,\n            sample_size=sample_size,\n        )\n        summary = replace_double_quotes_as_single_quotes(summary)\n        data = {\n            \"tags\": lambda: generate_tags(",
        "type": "code",
        "location": "/remove_unwanted_notes.py:607-636"
    },
    "107": {
        "file_id": 14,
        "content": "The code defines a function `remove_unwanted_notes.py` that takes missing fields and field-to-method dictionary as input, retrieves additional metadata for each missing field, and returns the purified dictionary of additional metadata. It also includes a generator function for new metadata with the possibility of changing the summary text based on content without metadata, word limit, programming language, character limit, line limit, and sample size.",
        "type": "comment"
    },
    "108": {
        "file_id": 14,
        "content": "                tags_similarity_index, summary, top_k=tag_top_k\n            ),\n            \"title\": lambda: generate_title(summary),\n            \"description\": lambda: generate_description(summary),\n            \"category\": lambda: generate_category(\n                categories_similarity_index, summary, top_k=category_top_k\n            ),\n        }\n        return data\n    def find_missing_fields_and_build_field_to_method():\n        field_to_method = {\n            \"date\": lambda: generate_date(filepath, metadata),\n        }\n        missing_fields = [\n            field for field in REQUIRED_FIELDS if field not in metadata.keys()\n        ]\n        if set(missing_fields).intersection(FIELDS_THAT_NEED_SUMMARY_TO_GENERATE):\n            field_to_method_with_summary = build_field_generation_methods_with_summary()\n            field_to_method.update(field_to_method_with_summary)\n        return missing_fields, field_to_method\n    def get_new_metadata_and_changed_flag():\n        (\n            missing_fields,\n            field_to_method,",
        "type": "code",
        "location": "/remove_unwanted_notes.py:637-662"
    },
    "109": {
        "file_id": 14,
        "content": "Function `remove_unwanted_notes.py` contains a function that generates metadata for a note based on its summary, and another function that finds missing fields and builds field-to-method mapping. It also has a variable containing required fields for the metadata.",
        "type": "comment"
    },
    "110": {
        "file_id": 14,
        "content": "        ) = find_missing_fields_and_build_field_to_method()\n        additional_metadata = get_additional_metadata(missing_fields, field_to_method)\n        new_metadata, changed = generate_new_metadata(additional_metadata)\n        return new_metadata, changed\n    return get_new_metadata_and_changed_flag()\n@beartype\ndef check_if_contains_bad_words(content: str, bad_words: list[str]):\n    for word in bad_words:\n        if word in content:\n            return True\n    return False\n# use two hashs for cache varification\n# store filename before and after processing\n# one before processing, one after processing\n# store processed ones to some place for cacheing\n# you need to collect existing tags and categories before processing\n# collect only from file without bad words.\ndef check_if_has_markdown_file_extension(filename: str):\n    return filename.endswith(\".md\")\ndef iterate_and_get_markdown_filepath_from_notedir(notes_dir: str):\n    for filename in os.listdir(notes_dir):\n        if check_if_has_markdown_file_extension(filename):",
        "type": "code",
        "location": "/remove_unwanted_notes.py:663-695"
    },
    "111": {
        "file_id": 14,
        "content": "Iterate through notes directory, filtering .md files.\nCheck if file contains bad words and get modified metadata with additional fields.\nStore before and after processed file for cache verification.",
        "type": "comment"
    },
    "112": {
        "file_id": 14,
        "content": "            source_path = os.path.join(notes_dir, filename)\n            yield source_path\n@beartype\ndef extract_and_update_existing_tags_and_categories(\n    content: str, existing_tags: set[str], existing_categories: set[str]\n):\n    has_metadata, metadata, _, _ = parse_content_metadata(content)\n    if has_metadata:\n        update_tags_and_categories_from_metadata(\n            metadata, existing_tags, existing_categories\n        )\n@beartype\ndef get_note_paths_without_bad_words_and_existing_tags_and_categories(\n    notes_dir: str, bad_words: list[str], cache_dir: str\n):\n    note_paths: list[str] = []\n    existing_tags: set[str] = set()\n    existing_categories: set[str] = set()\n    @beartype\n    def check_bad_words_passed(content: str, check_bad_words: bool):\n        if check_bad_words:\n            passed = not check_if_contains_bad_words(content, bad_words)\n        else:\n            passed = True\n        return passed\n    @beartype\n    def append_note_path_and_update_existing_tags_and_categories(\n        content: str,",
        "type": "code",
        "location": "/remove_unwanted_notes.py:696-729"
    },
    "113": {
        "file_id": 14,
        "content": "This code seems to be part of a function that extracts and updates existing tags and categories for notes, while also checking if the note content contains any bad words or unwanted tags. It returns a list of note paths along with updated tag and category information. The `check_bad_words_passed` function checks whether the note content contains any bad words.",
        "type": "comment"
    },
    "114": {
        "file_id": 14,
        "content": "        filepath: str,\n        append_and_check_bad_words: bool,\n    ):\n        if check_bad_words_passed(content, append_and_check_bad_words):\n            if append_and_check_bad_words:\n                note_paths.append(filepath)\n            extract_and_update_existing_tags_and_categories(\n                content, existing_tags, existing_categories\n            )\n    @beartype\n    def iterate_dir_and_update_tags_and_categoiries(\n        dirpath: str, append_and_check_bad_words: bool = True\n    ):\n        for fpath in iterate_and_get_markdown_filepath_from_notedir(dirpath):\n            content = load_file(fpath)\n            append_note_path_and_update_existing_tags_and_categories(\n                content, fpath, append_and_check_bad_words\n            )\n    iterate_dir_and_update_tags_and_categoiries(\n        notes_dir,\n    )\n    iterate_dir_and_update_tags_and_categoiries(\n        cache_dir, append_and_check_bad_words=False\n    )\n    return note_paths, existing_tags, existing_categories\ndef update_tags_set_from_metadata(metadata: JSONDict, tags_set: set[str]):",
        "type": "code",
        "location": "/remove_unwanted_notes.py:730-760"
    },
    "115": {
        "file_id": 14,
        "content": "This code is iterating over a directory of markdown files and updating existing tags and categories for each note file. If the append_and_check_bad_words flag is True, it appends the filepath to the note_paths list after checking if the content has bad words. It also extracts and updates existing tags and categories for the given content. The code then calls iterate_dir_and_update_tags_and_categories on two directories: notes_dir with append_and_check_bad_words set to True, and cache_dir with append_and_check_bad_words set to False. Finally, it returns the note_paths, existing_tags, and existing_categories. The update_tags_set_from_metadata function updates a tags_set from metadata in a JSONDict.",
        "type": "comment"
    },
    "116": {
        "file_id": 14,
        "content": "    for tag in metadata.get(\"tags\", []):\n        tags_set.add(tag)\ndef update_categories_set_from_metadata(metadata: JSONDict, categories_set: set[str]):\n    category = metadata.get(\"category\", None)\n    if category:\n        categories_set.add(category)\n@beartype\ndef update_tags_and_categories_from_metadata(\n    metadata: JSONDict, tags_set: set[str], categories_set: set[str]\n):\n    update_tags_set_from_metadata(metadata, tags_set)\n    update_categories_set_from_metadata(metadata, categories_set)\n@beartype\ndef process_note_content_with_similarity_indices(\n    content: str,\n    source_path: str,\n    tags_similarity_index: SimilarityIndex,\n    categories_similarity_index: SimilarityIndex,\n    sample_size: Optional[int] = None,\n):\n    (\n        has_metadata,\n        metadata,\n        content_without_metadata,\n        first_match,\n    ) = parse_content_metadata(content)\n    new_metadata, changed = generate_content_metadata(\n        source_path,\n        content_without_metadata,\n        metadata,\n        tags_similarity_index,",
        "type": "code",
        "location": "/remove_unwanted_notes.py:761-797"
    },
    "117": {
        "file_id": 14,
        "content": "This function takes a JSON dictionary of metadata and two sets of categories and tags. It adds the category from the metadata to the categories set if it exists, and iterates over each tag in the \"tags\" list in the metadata (if present), adding each one to the tags set.",
        "type": "comment"
    },
    "118": {
        "file_id": 14,
        "content": "        categories_similarity_index,\n        sample_size=sample_size,\n    )\n    if changed:\n        return modify_content_metadata(content, has_metadata, new_metadata, first_match)\n    return content\n@beartype\ndef process_and_write_note_with_similarity_indices(\n    source_path: str,\n    target_path: str,\n    tags_similarity_index: SimilarityIndex,\n    categories_similarity_index: SimilarityIndex,\n    sample_size: Optional[int] = None,\n):\n    content = load_file(source_path)\n    new_content = process_note_content_with_similarity_indices(\n        content,\n        source_path,\n        tags_similarity_index,\n        categories_similarity_index,\n        sample_size=sample_size,\n    )\n    write_file(target_path, new_content)\n@beartype\ndef get_existing_note_info_from_notes_dir_and_bad_words_path(\n    notes_dir: str, bad_words_path: str, cache_dir: str\n):\n    bad_words = load_bad_words(bad_words_path)\n    (\n        note_paths,\n        existing_tags,\n        existing_categories,\n    ) = get_note_paths_without_bad_words_and_existing_tags_and_categories(",
        "type": "code",
        "location": "/remove_unwanted_notes.py:798-834"
    },
    "119": {
        "file_id": 14,
        "content": "This code defines functions for processing and writing notes with similarity indices, getting existing note info from notes directory and bad words path. It loads content from source paths, processes the content using similarity indices, writes new content to target paths, gets note paths without bad words, and retrieves existing tags and categories.",
        "type": "comment"
    },
    "120": {
        "file_id": 14,
        "content": "        notes_dir, bad_words, cache_dir\n    )\n    return (note_paths, existing_tags, existing_categories)\n@beartype\ndef generate_processed_note_path(param: TargetGeneratorParameter):\n    basename = generate_markdown_name()\n    ret = os.path.join(param.target_dir_path, basename)\n    return ret\n@beartype\ndef generate_process_and_write_note_method(\n    tags_similarity_index: SimilarityIndex,\n    categories_similarity_index: SimilarityIndex,\n    sample_size: Optional[int] = None,\n):\n    @beartype\n    def process_and_write_note(\n        source_path: str,\n        target_path: str,\n    ):\n        return process_and_write_note_with_similarity_indices(\n            source_path,\n            target_path,\n            tags_similarity_index,\n            categories_similarity_index,\n            sample_size=sample_size,\n        )\n    return process_and_write_note\n@beartype\ndef generate_source_walker_from_note_paths(note_paths: list[str]):\n    def source_walker(dirpath: str):\n        for fpath in note_paths:\n            yield dirpath, fpath",
        "type": "code",
        "location": "/remove_unwanted_notes.py:835-873"
    },
    "121": {
        "file_id": 14,
        "content": "1. Generates a name for the processed note file\n2. Returns the full path to the processed note file in the target directory\n3. Creates a function that processes and writes a note, using similarity indices for tags and categories (if provided)\n4. Returns a function that can be used as a source walker, yielding paths of the note files from the given list of note paths",
        "type": "comment"
    },
    "122": {
        "file_id": 14,
        "content": "    return source_walker\n@beartype\ndef prepare_note_iterator_extra_params(\n    note_paths: list[str],\n    sent_trans_model: SentenceTransformer,\n    existing_tags: set[str],\n    existing_categories: set[str],\n    sample_size: Optional[int] = None,\n):\n    @beartype\n    def create_similarity_index_with_candidates(candidates: Iterable[str]):\n        return SimilarityIndex(sent_trans_model, candidates)\n    def get_tags_and_categories_similarity_indices():\n        tags_similarity_index = create_similarity_index_with_candidates(existing_tags)\n        categories_similarity_index = create_similarity_index_with_candidates(\n            existing_categories\n        )\n        return tags_similarity_index, categories_similarity_index\n    def generate_source_walker_and_target_file_generator():\n        (\n            tags_similarity_index,\n            categories_similarity_index,\n        ) = get_tags_and_categories_similarity_indices()\n        source_walker = generate_source_walker_from_note_paths(note_paths)\n        target_file_generator = generate_process_and_write_note_method(",
        "type": "code",
        "location": "/remove_unwanted_notes.py:875-904"
    },
    "123": {
        "file_id": 14,
        "content": "Function returns a source walker and a target file generator from given note paths, using existing tags and categories similarity indices to filter out unwanted notes.",
        "type": "comment"
    },
    "124": {
        "file_id": 14,
        "content": "            tags_similarity_index, categories_similarity_index, sample_size=sample_size\n        )\n        return source_walker, target_file_generator\n    return generate_source_walker_and_target_file_generator()\n@beartype\ndef iterate_note_paths_without_bad_words_and_write_to_cache(\n    param: SourceIteratorAndTargetGeneratorParam,\n    note_paths: list[str],\n    existing_tags: set[str],\n    existing_categories: set[str],\n    sample_size: Optional[int] = None,\n) -> list[str]:\n    with sentence_transformer_context() as sent_trans_model:\n        source_walker, target_file_generator = prepare_note_iterator_extra_params(\n            note_paths,\n            sent_trans_model,\n            existing_tags,\n            existing_categories,\n            sample_size=sample_size,\n        )\n        return iterate_source_dir_and_generate_to_target_dir(\n            param,\n            source_walker=source_walker,\n            target_path_generator=generate_processed_note_path,\n            target_file_geneator=target_file_generator,",
        "type": "code",
        "location": "/remove_unwanted_notes.py:905-932"
    },
    "125": {
        "file_id": 14,
        "content": "This code defines a function that iterates over note paths, skips any with bad words, and writes the remaining notes to cache. The function uses a SentenceTransformer model for similarity calculations and requires additional parameters such as existing tags, categories, and sample size (optional). It also includes a nested function that prepares the source iterator and target file generator.",
        "type": "comment"
    },
    "126": {
        "file_id": 14,
        "content": "            join_source_dir=False,\n        )\n@beartype\ndef walk_notes_source_dir_and_write_to_cache_dir(\n    param: SourceIteratorAndTargetGeneratorParam,\n    bad_words_path: str,\n    sample_size: Optional[int] = None,\n):\n    (\n        note_paths,\n        existing_tags,\n        existing_categories,\n    ) = get_existing_note_info_from_notes_dir_and_bad_words_path(\n        param.source_dir_path, bad_words_path, param.target_dir_path\n    )\n    return iterate_note_paths_without_bad_words_and_write_to_cache(\n        param, note_paths, existing_tags, existing_categories, sample_size=sample_size\n    )\n@beartype\ndef remove_and_create_dir(dirpath: str):\n    if os.path.exists(dirpath):\n        shutil.rmtree(dirpath)\n    os.mkdir(dirpath)\n@beartype\ndef fix_date_and_get_title_in_content(filepath: str, content: str) -> tuple[str, str]:\n    def fix_date_and_get_title():\n        (\n            has_metadata,\n            metadata,\n            _,\n            first_match,\n        ) = parse_content_metadata(content)\n        @beartype\n        def get_new_content_and_title() -> tuple[str, str]:",
        "type": "code",
        "location": "/remove_unwanted_notes.py:933-973"
    },
    "127": {
        "file_id": 14,
        "content": "This code defines several functions related to working with note files and directories. The `walk_notes_source_dir_and_write_to_cache_dir` function walks through the source directory, excluding any file paths containing \"bad words\", and writes the valid file paths to the cache directory. The `remove_and_create_dir` function removes an existing directory if it exists, then creates a new one at the same path. The `fix_date_and_get_title_in_content` function parses content metadata from a file's content and returns the new content with fixed date format and title.",
        "type": "comment"
    },
    "128": {
        "file_id": 14,
        "content": "            title = metadata.get(\"title\", \"\")\n            date = generate_date(filepath, metadata)\n            metadata[\"date\"] = date\n            new_content = modify_content_metadata(\n                content, has_metadata, metadata, first_match\n            )\n            return new_content, title\n        def process_parsed_metadata():\n            title = \"\"\n            new_content = content\n            if has_metadata:\n                if metadata is not None:\n                    new_content, title = get_new_content_and_title()\n            return new_content, title\n        return process_parsed_metadata()\n    return fix_date_and_get_title()\n@beartype\ndef remove_headline_from_content(content: str, title: str):\n    lines = split_by_line(content)\n    new_lines = remove_headline_from_lines(lines, title)\n    return join_lines_with_state(new_lines)\n@beartype\ndef reformat_title(content: str, title: str):\n    return content.replace(title, title.title(), 1)\n@beartype\ndef fix_date_in_cache_and_write_to_final_dir(\n    processed_cache_paths: list[str], final_dir: str",
        "type": "code",
        "location": "/remove_unwanted_notes.py:974-1009"
    },
    "129": {
        "file_id": 14,
        "content": "This code seems to be part of a larger program that processes and formats text content. It includes functions for modifying content with metadata, processing parsed metadata, removing headlines from lines, reformat title, and fixing the date in cache and writing to final directory. The specific code snippet appears to return the new content and title after processing.",
        "type": "comment"
    },
    "130": {
        "file_id": 14,
        "content": "):\n    remove_and_create_dir(final_dir)\n    for path in processed_cache_paths:\n        content = load_file(path)\n        content, title = fix_date_and_get_title_in_content(path, content)\n        content = remove_headline_from_content(content, title)\n        content = reformat_title(content, title)\n        new_path = os.path.join(final_dir, os.path.basename(path))\n        write_file(new_path, content)\n@beartype\ndef parse_params() -> tuple[SourceIteratorAndTargetGeneratorParam, str, str, int]:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--notes-source-dir\", type=str, default=\"notes\")\n    parser.add_argument(\"--cache-dir\", type=str, default=\"cache\")\n    parser.add_argument(\"--final-dir\", type=str, default=\"source/_posts\")\n    parser.add_argument(\"--db-path\", type=str, default=\"cache_db.json\")\n    parser.add_argument(\"--bad-words-path\", type=str, default=\"bad_words.txt\")\n    parser.add_argument(\"--sample-size\", type=int, default=10)\n    args = parser.parse_args()\n    param = SourceIteratorAndTargetGeneratorParam(",
        "type": "code",
        "location": "/remove_unwanted_notes.py:1010-1032"
    },
    "131": {
        "file_id": 14,
        "content": "This code is parsing command line arguments for a program. It sets default values for the notes source directory, cache directory, final directory, database file path, bad words file path, and sample size. The parse_params function returns these values as a tuple of type SourceIteratorAndTargetGeneratorParam, string, string, and integer.",
        "type": "comment"
    },
    "132": {
        "file_id": 14,
        "content": "        source_dir_path=args.notes_source_dir,\n        target_dir_path=args.cache_dir,\n        db_path=args.db_path,\n    )\n    return param, args.bad_words_path, args.final_dir, args.sample_size\ndef main():\n    param, bad_words_path, final_dir, sample_size = parse_params()\n    processed_cache_paths = walk_notes_source_dir_and_write_to_cache_dir(\n        param, bad_words_path, sample_size=sample_size\n    )\n    fix_date_in_cache_and_write_to_final_dir(processed_cache_paths, final_dir)\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/remove_unwanted_notes.py:1033-1049"
    },
    "133": {
        "file_id": 14,
        "content": "This code is parsing command line parameters, then walking through a source directory, processing notes and writing them to a cache directory. Finally, it fixes the date in the cache files and writes them to a final output directory.",
        "type": "comment"
    },
    "134": {
        "file_id": 15,
        "content": "/serve.sh",
        "type": "filepath"
    },
    "135": {
        "file_id": 15,
        "content": "Deletes db.json, generates static files with Hexo, starts a local web server on port 8021 for serving the generated files.",
        "type": "summary"
    },
    "136": {
        "file_id": 15,
        "content": "rm db.json\nhexo generate\ncd public\npython3 -m http.server 8021",
        "type": "code",
        "location": "/serve.sh:1-4"
    },
    "137": {
        "file_id": 15,
        "content": "Deletes db.json, generates static files with Hexo, starts a local web server on port 8021 for serving the generated files.",
        "type": "comment"
    },
    "138": {
        "file_id": 16,
        "content": "/similarity_utils.py",
        "type": "filepath"
    },
    "139": {
        "file_id": 16,
        "content": "The code utilizes SentenceTransformers for cosine similarity and generates titles, categories, and tags. It involves a class for embedding computations, indexing, searching through an embedding index, and preparing search results using pre-trained models.",
        "type": "summary"
    },
    "140": {
        "file_id": 16,
        "content": "from contextlib import contextmanager\nimport os\nfrom typing import Iterable, Optional, Union, overload, Literal\nfrom beartype import beartype\nimport weakref\nimport progressbar\nimport torch\n# use sys.path.append to insert dependencies\n# ask llm to give some potential tags & category for content chunks\n# calculate cosine similarity to existing content\n# ask the llm to use existing tag & category or create new ones.\n# check if the newly created tag & category exists and update\n# to create title:\n# summarize the content\n# generate title from summary\n# get the time:\n# usr rclone to preserve timestamp\n# get mtime from file metadata\n# set mirror path\nos.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\nimport sentence_transformers  # i recommend you to use cpu only.\nimport sentence_transformers.util\nfrom contextlib import contextmanager\n@contextmanager\ndef sentence_transformer_context(\n    model_name=\"distiluse-base-multilingual-cased-v1\", device=\"cpu\", **kwargs\n):\n    model = sentence_transformers.SentenceTransformer(\n        model_name, device=device, **kwargs",
        "type": "code",
        "location": "/similarity_utils.py:1-36"
    },
    "141": {
        "file_id": 16,
        "content": "This code imports necessary libraries and sets up a context manager for using the SentenceTransformers model. It allows for calculating cosine similarity between content chunks, generating titles based on content summaries, preserving file metadata timestamps, and possibly categorizing content with tags and categories.",
        "type": "comment"
    },
    "142": {
        "file_id": 16,
        "content": "    )\n    try:\n        yield model\n    finally:\n        del model\n@beartype\nclass SimilarityIndex(object):\n    def __init__(\n        self,\n        model: sentence_transformers.SentenceTransformer,\n        candidates: Iterable[str] = [],\n    ):\n        self.init_properties(model)\n        self.insert_multiple_candidates(candidates)\n    def init_index(self):\n        self.word_index: list[str] = []\n        self.embedding_index: Optional[torch.Tensor] = None\n    def init_properties(self, model: sentence_transformers.SentenceTransformer):\n        self.init_index()\n        self._model_weakref_ = weakref.ref(model)\n    @property\n    def model(self):\n        return self._model_weakref_()\n    def encode_multiple(self, items: list[str]):\n        embed_list = []\n        for it in progressbar.progressbar(items):\n            embed_list.append(self.encode_single(it))\n        ret = torch.cat(embed_list, dim=0)\n        return ret\n    def encode_single(self, it: str):\n        embed: torch.Tensor = self.model.encode([it], convert_to_tensor=True)  # type: ignore",
        "type": "code",
        "location": "/similarity_utils.py:37-74"
    },
    "143": {
        "file_id": 16,
        "content": "The code initializes a SimilarityIndex object with a sentence transformer model and a list of candidates. It also creates an index, encodes multiple strings into embeddings, and encodes a single string into an embedding.",
        "type": "comment"
    },
    "144": {
        "file_id": 16,
        "content": "        return embed\n    def encode(self, it: Union[str, list[str]]):\n        if isinstance(it, str):\n            return self.encode_single(it)\n        else:\n            return self.encode_multiple(it)\n    def update_embedding_index(self, embed: torch.Tensor):\n        if self.embedding_index is None:\n            self.embedding_index = embed\n        else:\n            self.embedding_index = torch.cat([self.embedding_index, embed], dim=0)\n    def insert_single_candidate(self, candidate: str):\n        it = candidate.strip()\n        if it:\n            if it not in self.word_index:\n                self.word_index.append(it)\n                embed = self.encode_single(it)\n                self.update_embedding_index(embed)\n    def insert_multiple_candidates(self, candidates: Iterable[str]):\n        for it in progressbar.progressbar(candidates):\n            self.insert_single_candidate(it)\n    def compute_similarity(self, it: Union[str, list[str]]):\n        if self.embedding_index is None:\n            raise Exception(\"No embedding index yet. Cannot compute similarity.\")",
        "type": "code",
        "location": "/similarity_utils.py:75-103"
    },
    "145": {
        "file_id": 16,
        "content": "The code defines a class that handles embedding computations and indexing for similarity calculations. It includes methods to encode single or multiple strings into embeddings, update the embedding index, insert single or multiple candidates into the index, and compute similarities based on the embedding index.",
        "type": "comment"
    },
    "146": {
        "file_id": 16,
        "content": "        embed = self.encode(it)\n        similarity = sentence_transformers.util.cos_sim(embed, self.embedding_index)\n        similarity = torch.sum(similarity, dim=0)\n        return similarity\n    # first overload should agree with default keyword arguments\n    @overload\n    def search(\n        self,\n        query: Union[str, list[str]],\n        top_k: int = 10,\n        return_similarity: Literal[False] = False,\n    ) -> list[str]:\n        ...\n    @overload\n    def search(\n        self,\n        query: Union[str, list[str]],\n        top_k: int = 10,\n        return_similarity: Literal[True] = True,\n    ) -> dict[str, float]:\n        ...\n    def search(\n        self,\n        query,\n        top_k=10,\n        return_similarity=False,\n    ):\n        query_length, similarity_list, top_k_indices = self.get_similarity_info(\n            query, top_k\n        )\n        ret = self.prepare_search_results(\n            query_length, similarity_list, top_k_indices, return_similarity\n        )\n        return ret\n    def prepare_search_results(",
        "type": "code",
        "location": "/similarity_utils.py:104-142"
    },
    "147": {
        "file_id": 16,
        "content": "This code defines a class with methods for searching through an embedding index. The `search` method takes a query and optional parameters for the number of top results (top_k) and whether to return similarity scores (return_similarity). If no similarity is requested, it returns a list of matching results; if similarity is requested, it returns a dictionary with the index and similarity score for each result. The `get_similarity_info` and `prepare_search_results` methods help prepare the search results based on the input parameters.",
        "type": "comment"
    },
    "148": {
        "file_id": 16,
        "content": "        self,\n        query_length: int,\n        similarity_list: list[float],\n        top_k_indices: list[int],\n        return_similarity: bool,\n    ):\n        if return_similarity:\n            return {\n                self.word_index[ind]: similarity_list[ind] / query_length\n                for ind in top_k_indices\n            }\n        else:\n            return [self.word_index[ind] for ind in top_k_indices]\n    def can_compute_similarity(self, query: Union[str, list[str]]):\n        query_length = 1 if isinstance(query, str) else len(query)\n        can_compute = self.index_size > 0 and query_length > 0\n        return can_compute, query_length\n    def compute_similarity_and_get_top_k_indices(\n        self, query: Union[str, list[str]], top_k: int = 10\n    ):\n        similarity = self.compute_similarity(query)\n        similarity_list = similarity.tolist()\n        top_k_indices = self.get_top_k_indices(similarity, top_k)\n        return similarity_list, top_k_indices\n    def get_similarity_info(self, query: Union[str, list[str]], top_k: int = 10):",
        "type": "code",
        "location": "/similarity_utils.py:143-170"
    },
    "149": {
        "file_id": 16,
        "content": "1. Function returns a dictionary of word indices and their normalized similarity scores for top_k indices\n2. Function checks if index size is not zero and query length is not zero before returning True or False\n3. Computes the similarity and converts it to a list for further processing\n4. Gets top_k indices from the computed similarity\n5. Retrieves information related to similarity and top_k indices using get_similarity_info function",
        "type": "comment"
    },
    "150": {
        "file_id": 16,
        "content": "        similarity_list = []\n        top_k_indices = []\n        can_compute, query_length = self.can_compute_similarity(query)\n        if can_compute:\n            (\n                similarity_list,\n                top_k_indices,\n            ) = self.compute_similarity_and_get_top_k_indices(query, top_k)\n        return query_length, similarity_list, top_k_indices\n    @property\n    def index_size(self):\n        return len(self.word_index)\n    def get_top_k_indices(self, similarity: torch.Tensor, top_k=10):\n        if self.index_size <= top_k:\n            top_k_indices = list(range(self.index_size))\n        else:\n            top_k_indices = torch.topk(similarity, top_k).indices.squeeze().tolist()\n        return top_k_indices\ndef test_main():\n    texts = [\"I love my pet dog and spend a lot of time with it.\", \"我爱我的宠物狗，并且经常和它在一起。\"]\n    tags = [\n        \"dogs\",\n        \"cats\",\n        \"computer\",\n        \"tech\",\n        \"life\",\n        \"dress\",\n        \"cook\",\n        \"outfit\",\n        \"fixing\",\n        \"mechanics\",\n        \"car\",",
        "type": "code",
        "location": "/similarity_utils.py:171-207"
    },
    "151": {
        "file_id": 16,
        "content": "Code snippet is a part of a class that computes similarity between queries and text data. It checks if the query length is computable, then calculates similarity scores and top k indices using `compute_similarity_and_get_top_k_indices` method. If the size of index is less than or equal to top_k, it returns all indices, else it uses torch's topk function to get top_k indices.",
        "type": "comment"
    },
    "152": {
        "file_id": 16,
        "content": "        \"gasoline\",\n    ]\n    with sentence_transformer_context() as model:\n        sim_index = SimilarityIndex(model, candidates=tags)\n        for it in texts:\n            ret = sim_index.search(query=it)\n            print(it, \"->\", ret)\nif __name__ == \"__main__\":\n    test_main()",
        "type": "code",
        "location": "/similarity_utils.py:208-220"
    },
    "153": {
        "file_id": 16,
        "content": "Iterating through each text, using a pre-trained model for similarity search, and printing the results.",
        "type": "comment"
    },
    "154": {
        "file_id": 17,
        "content": "/sitemap_template.html.j2",
        "type": "filepath"
    },
    "155": {
        "file_id": 17,
        "content": "This code is an HTML template for a website sitemap, including basic meta tags, title, and a list of pages with their respective URLs.",
        "type": "summary"
    },
    "156": {
        "file_id": 17,
        "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Website Sitemap</title>\n</head>\n<body>\n    <h1>Website Sitemap</h1>\n    <ul>\n        <li><a href=\"/\">Home</a></li>\n{% for index, url in datalist %}\n        <li><a href=\"{{url}}\">Page {{index}}</a></li>\n{% endfor %}\n    </ul>\n</body>\n</html>",
        "type": "code",
        "location": "/sitemap_template.html.j2:1-17"
    },
    "157": {
        "file_id": 17,
        "content": "This code is an HTML template for a website sitemap, including basic meta tags, title, and a list of pages with their respective URLs.",
        "type": "comment"
    },
    "158": {
        "file_id": 18,
        "content": "/sitemap_template.xml.j2",
        "type": "filepath"
    },
    "159": {
        "file_id": 18,
        "content": "This code generates an XML sitemap file in the specified format for SEO purposes, using data from a \"datalist\" variable.",
        "type": "summary"
    },
    "160": {
        "file_id": 18,
        "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset\n      xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"\n      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n      xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9\n            http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\">\n<!-- created with Free Online Sitemap Generator www.xml-sitemaps.com -->\n{% for url_loc, lastmod, priority in datalist %}\n<url>\n  <loc>{{url_loc}}</loc>\n  <lastmod>{{lastmod}}</lastmod>\n  <priority>{{priority}}</priority>\n</url>\n{% endfor %}\n</urlset>",
        "type": "code",
        "location": "/sitemap_template.xml.j2:1-16"
    },
    "161": {
        "file_id": 18,
        "content": "This code generates an XML sitemap file in the specified format for SEO purposes, using data from a \"datalist\" variable.",
        "type": "comment"
    },
    "162": {
        "file_id": 19,
        "content": "/sync_quarto.sh",
        "type": "filepath"
    },
    "163": {
        "file_id": 19,
        "content": "Code executes script sync_to_quarto.py, then navigates to quarto_blog folder, and runs preview.sh script.",
        "type": "summary"
    },
    "164": {
        "file_id": 19,
        "content": "python3 sync_to_quarto.py\ncd quarto_blog\nbash preview.sh",
        "type": "code",
        "location": "/sync_quarto.sh:1-3"
    },
    "165": {
        "file_id": 19,
        "content": "Code executes script sync_to_quarto.py, then navigates to quarto_blog folder, and runs preview.sh script.",
        "type": "comment"
    },
    "166": {
        "file_id": 20,
        "content": "/sync_to_quarto.py",
        "type": "filepath"
    },
    "167": {
        "file_id": 20,
        "content": "The code reads content strings, parses and modifies their metadata, removes \"created\" and \"modified\" keys and replaces `'` with &grave;. It syncs source directory content to Quarto format in a target directory, creating subdirectories based on filenames without extension, fixes line wrapping issues, and writes new content as .qmd files.",
        "type": "summary"
    },
    "168": {
        "file_id": 20,
        "content": "source_dir = \"source/_posts\"\ntarget_dir = \"quarto_blog/myblog/posts\"\nimport os\nimport shutil\nfrom headline_match import parse_content_metadata, dump_dict_as_yaml\nfrom beartype import beartype\nfrom io_utils import load_file, write_file\n@beartype\ndef fix_metadata_line_wrap_in_content(content: str):\n    (\n        has_metadata,\n        metadata,\n        content_without_metadata,\n        first_match,\n    ) = parse_content_metadata(content)\n    if has_metadata:\n        if metadata is not None:\n            for k in [\"created\", \"modified\"]:\n                if k in metadata.keys():\n                    del metadata[k]\n            keys = list(metadata.keys())\n            for k in keys:\n                v = metadata[k]\n                if isinstance(v, str):\n                    metadata[k] = v.replace(\"`\", \"&grave;\")\n                elif isinstance(v, list):\n                    metadata[k] = [\n                        x if not isinstance(x, str) else x.replace(\"`\", \"&grave;\")\n                        for x in v\n                    ]",
        "type": "code",
        "location": "/sync_to_quarto.py:1-33"
    },
    "169": {
        "file_id": 20,
        "content": "The code reads a content string, parses its metadata, modifies the metadata by removing \"created\" and \"modified\" keys and replacing occurrences of `'` with &grave; in strings or lists, and then returns the modified content and metadata.",
        "type": "comment"
    },
    "170": {
        "file_id": 20,
        "content": "            metadata[\"categories\"] = metadata.get(\"tags\", [])\n            repl = dump_dict_as_yaml(metadata)\n            new_content = (\n                \"---\\n\"\n                + repl\n                + \"\\n---\\n\"\n                + (\"\\n\" + content_without_metadata + \"\\n\").replace(\n                    \"\\n---\\n\", \"\\n------\\n\"\n                )\n            )\n            return new_content\n    return content\n# you need to convert multiline yaml into one\nif os.path.exists(target_dir):\n    shutil.rmtree(target_dir)\nos.mkdir(target_dir)\nfor fname in os.listdir(source_dir):\n    source_path = os.path.join(source_dir, fname)\n    content = load_file(source_path)\n    new_content = fix_metadata_line_wrap_in_content(content)\n    identifier = fname.split(\".\")[0]\n    target_subdir = os.path.join(target_dir, identifier)\n    os.mkdir(target_subdir)\n    target_path = os.path.join(target_subdir, \"index.qmd\")\n    write_file(target_path, new_content)",
        "type": "code",
        "location": "/sync_to_quarto.py:34-62"
    },
    "171": {
        "file_id": 20,
        "content": "Code syncs content from a source directory to Quarto format in a target directory. It first checks if the target directory exists, removes it if it does, and then creates it. For each file in the source directory, it loads the content, fixes any line wrapping issues in the metadata section, and assigns an identifier based on the filename without extension. It then creates a subdirectory in the target directory for each identifier and writes the new content as a Quarto (.qmd) file in the corresponding subdirectory.",
        "type": "comment"
    },
    "172": {
        "file_id": 21,
        "content": "/test_parse.py",
        "type": "filepath"
    },
    "173": {
        "file_id": 21,
        "content": "Converts input date string to datetime object",
        "type": "summary"
    },
    "174": {
        "file_id": 21,
        "content": "from parse import parse\ninput_date_string = \"2022-11-28-22-01-29\"\n# Define the format to parse the input string\ninput_format = \"{year:d}-{month:d}-{day:d}-{hour:d}-{minute:d}-{second:d}\"\n# Parse the input string using the specified format\nparsed = parse(input_format, input_date_string)\nprint(type(parsed))",
        "type": "code",
        "location": "/test_parse.py:1-9"
    },
    "175": {
        "file_id": 21,
        "content": "Converts input date string to datetime object",
        "type": "comment"
    },
    "176": {
        "file_id": 22,
        "content": "/update_posts.sh",
        "type": "filepath"
    },
    "177": {
        "file_id": 22,
        "content": "Updating API key and base URL, enabling better exceptions, then running the script.",
        "type": "summary"
    },
    "178": {
        "file_id": 22,
        "content": "export OPENAI_API_KEY='any'\nexport OPENAI_API_BASE=http://0.0.0.0:8000\nexport BETTER_EXCEPTIONS=1\npython3 remove_unwanted_notes.py",
        "type": "code",
        "location": "/update_posts.sh:2-6"
    },
    "179": {
        "file_id": 22,
        "content": "Updating API key and base URL, enabling better exceptions, then running the script.",
        "type": "comment"
    },
    "180": {
        "file_id": 23,
        "content": "/update_target_hash.py",
        "type": "filepath"
    },
    "181": {
        "file_id": 23,
        "content": "Updates target hash in the database for files present in cache directory.",
        "type": "summary"
    },
    "182": {
        "file_id": 23,
        "content": "import os\nimport tinydb\nimport sys\nsys.path.append(\n    \"/media/root/Toshiba XG3/works/prometheous/document_agi_computer_control\"\n)\nfrom cache_db_context import hash_file\ncache_path = \"cache/\"\ndb_path = \"cache_db.json\"\ndb = tinydb.TinyDB(db_path)\nfor fname in os.listdir(cache_path):\n    fpath = os.path.join(cache_path, fname)\n    q = tinydb.Query().target.path == fpath\n    it = db.get(q)\n    if it is not None:\n        new_it = it.copy()\n        new_it['target']['hash'] = hash_file(fpath)\n        db.upsert(new_it, cond = q)",
        "type": "code",
        "location": "/update_target_hash.py:1-22"
    },
    "183": {
        "file_id": 23,
        "content": "Updates target hash in the database for files present in cache directory.",
        "type": "comment"
    },
    "184": {
        "file_id": 24,
        "content": "/quarto_blog/init.sh",
        "type": "filepath"
    },
    "185": {
        "file_id": 24,
        "content": "Creating a Quarto project with the name \"myblog\"",
        "type": "summary"
    },
    "186": {
        "file_id": 24,
        "content": "quarto create project blog myblog",
        "type": "code",
        "location": "/quarto_blog/init.sh:1-1"
    },
    "187": {
        "file_id": 24,
        "content": "Creating a Quarto project with the name \"myblog\"",
        "type": "comment"
    },
    "188": {
        "file_id": 25,
        "content": "/quarto_blog/preview.sh",
        "type": "filepath"
    },
    "189": {
        "file_id": 25,
        "content": "Removing existing site files and previewing the blog.",
        "type": "summary"
    },
    "190": {
        "file_id": 25,
        "content": "rm -rf quarto_blog/myblog/_site\nquarto preview myblog --log-level info --log preview.log",
        "type": "code",
        "location": "/quarto_blog/preview.sh:1-2"
    },
    "191": {
        "file_id": 25,
        "content": "Removing existing site files and previewing the blog.",
        "type": "comment"
    }
}