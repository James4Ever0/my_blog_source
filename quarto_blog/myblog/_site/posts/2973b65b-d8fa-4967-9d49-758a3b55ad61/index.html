<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-08-06">
<meta name="description" content="This article covers deep learning on MacOS, exploring the use of M-series processors and AMD GPUs for training. It discusses training with a MacBook Air, using TensorFlow Metal and PyTorch with MPS. The article also delves into CoreML image analysis techniques and AutoML for Apple’s M1 chips. Additionally, it covers image/video analysis methods, Optical Flow, Person Segmentation, NLP APIs, sentiment analysis, speech recognition, and sound classification using DNNs.">

<title>James’ Blog - Deeplearning On Macos M-Series Processors</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">James’ Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/james4ever0" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Deeplearning On Macos M-Series Processors</h1>
  <div class="quarto-categories">
    <div class="quarto-category">CoreML</div>
    <div class="quarto-category">darling</div>
    <div class="quarto-category">hackintosh</div>
    <div class="quarto-category">paddlepaddle</div>
    <div class="quarto-category">Swift</div>
  </div>
  </div>

<div>
  <div class="description">
    This article covers deep learning on MacOS, exploring the use of M-series processors and AMD GPUs for training. It discusses training with a MacBook Air, using TensorFlow Metal and PyTorch with MPS. The article also delves into CoreML image analysis techniques and AutoML for Apple’s M1 chips. Additionally, it covers image/video analysis methods, Optical Flow, Person Segmentation, NLP APIs, sentiment analysis, speech recognition, and sound classification using DNNs.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 6, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<hr>
<p>it is funny that macOS still supports AMD GPUs, means any intel Mac (not M-series!) can now utilize internal/external AMD GPUs as long as frameworks like <a href="https://developer.apple.com/metal/jax/">jax</a> and <a href="https://pytorch.org/docs/stable/notes/mps.html">pytorch</a> support MPS/Metal.</p>
<hr>
<p>calling python code from swift using pythonkit:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">func</span> <span class="fu">downloadVideo</span><span class="op">(</span><span class="va">link</span><span class="op">:</span> <span class="dt">String</span><span class="op">){</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> <span class="va">sys</span> <span class="op">=</span> Python<span class="op">.</span><span class="kw">import</span>("<span class="im">sys</span>")</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sys<span class="op">.</span>path<span class="op">.</span>append<span class="op">(</span>dirPath<span class="op">)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> <span class="va">example</span> <span class="op">=</span> Python<span class="op">.</span><span class="kw">import</span>("<span class="im">sample</span>")</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">let</span> <span class="va">response</span> <span class="op">=</span> example<span class="op">.</span>downloadVideo<span class="op">(</span>link<span class="op">,</span> dirPath<span class="op">)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>videoPath <span class="op">=</span> String<span class="op">(</span>response<span class="op">)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="https://github.com/sickcodes/Docker-OSX">run macos in docker</a> with kvm</p>
<section id="neural-engine" class="level2">
<h2 class="anchored" data-anchor-id="neural-engine">neural engine</h2>
<p>it is used for coreml inference, not training</p>
</section>
<section id="run-coreml-on-hackintosh" class="level2">
<h2 class="anchored" data-anchor-id="run-coreml-on-hackintosh">run coreml on hackintosh</h2>
<p>first, download macos montery using the mac.</p>
<p>then, install it on hackintosh, with associated nvidia drivers.</p>
<p>next test gpu avalibility via system info panel.</p>
<p>then install xcode commandline tools and check coreml avalibility</p>
</section>
<section id="run-coreml-with-swift-on-linux" class="level2">
<h2 class="anchored" data-anchor-id="run-coreml-with-swift-on-linux">run coreml with swift on linux</h2>
<p>darling is at its very premature stage, just like the wine. now it is testing something called “darlingserver” which is a full userspace implementation and is prone to tons of problems. swift repl is not working and installing xcode commandline tools 14 will hang this thing. i suggest you to do light model training on macbook air and convert it to onnx if want to use it everywhere.</p>
<p>before reinstallation of darling, make sure you have removed all darling related files by checking <code>updatedb; locate darling | grep -v &lt;compile directory&gt;</code></p>
<p>visit <a href="https://docs.darlinghq.org/build-instructions.html">here</a> to install darling from source (maybe that’s the only way)</p>
<p>if want to install darling on kali, you must outsource all deeplearning models to other disks, and collect all other big files to somewhere else or trash them. use systemwide user broadcast method to warn me if any of the disk is missing. use automatic symlink change method to adapt the external disk mountpoint changes.</p>
<p>darling can <a href="https://github.com/darlinghq/darling-docs/blob/master/src/installing-software.md#:~:text=To%20install%20command-line%20developer%20tools%20such%20as%20the,install%20only%20command-line%20tools%20from%20Apple%20by%20running">install xcode commandline tools with macos sdk</a>, so maybe it can run coreml models with swift using cpu. gpu support is currently not known. maybe that requires metal support.</p>
</section>
<section id="thermal-and-battery-life-concerns-and-more" class="level2">
<h2 class="anchored" data-anchor-id="thermal-and-battery-life-concerns-and-more">thermal and battery life concerns, and more</h2>
<p>consider <a href="https://support.apple.com/en-us/HT208544">using external gpus (eGPUs) with thunderbolt 3 and AMD GPUs</a> to avoid overheating. currently that can only be done with intel Macs.</p>
<p>battery life is currently bad for intel/amd notebooks of x86/64 architecture.</p>
<p>heavy lifting jobs are likely to be run on Mac Studio with M1 Ultra and 128GB RAM. Macbook Air M1 with 8GB RAM is simply not feasible.</p>
<p>aside of Apple platforms, these APIs are virtually useless.</p>
<p>to run these on other non-apple machines, you need to tweak and install macOS on x86-64 platforms with macOS supported GPUs(may have low performance), which will definitely not taking any advantage of huge shared RAM with CPU, and may run poorly on CoreML/CreateML, may not support deepspeed stage 2/3 or BMI(big model inference)</p>
<details>
<summary>
Non-Supported NVIDIA Cards, use AMD GPU instead
</summary>
<p>High Sierra no longer supports NVIDIA Mac.</p>
<p>Mojave – Catalina – BigSur only works with AMD graphics and Intel onboard graphics and only a very small number of old NVIDIA products. Suppose you have GTX 1070, 1080, and the like, you can not use High Sierra onwards because Nvidia does not provide any updates for Mac and can not be used in any other way.</p>
<p>In general, the graphics of the Turing, Pascal, and Maxwell series will never be supported again. The latest Mac version that can use this series of graphics is High Sierra.</p>
</details>
</section>
<section id="tensorflow-with-m1-support" class="level2">
<h2 class="anchored" data-anchor-id="tensorflow-with-m1-support">tensorflow with m1 support</h2>
<p>using <a href="https://developer.apple.com/metal/tensorflow-plugin/">tensorflow metal plugin</a>, which sets up miniforge and install tensorflow-metal within.</p>
<p>install without miniforge(works!)</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install tensorflow-macos tensorflow-metal</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>validation:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-c</span> <span class="st">"import tensorflow as tf; physical_devices = tf.config.list_physical_devices('GPU'); print('Num GPUs:', len(physical_devices)); print(physical_devices)"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pytorch-with-m1-support-using-mps-metal-performance-shader" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-with-m1-support-using-mps-metal-performance-shader">pytorch with m1 support, using MPS (Metal performance shader)</h2>
<p>install from nightly release channel, with minimum system version requirements 12.3 (which this machine had been qualified after system update, now 12.5)</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MPS acceleration is available on MacOS 12.3+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install <span class="at">--pre</span> torch torchvision torchaudio <span class="at">--extra-index-url</span> https://download.pytorch.org/whl/nightly/cpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>validation</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-c</span> <span class="st">"import torch; print('MPS avaliable:',torch.backends.mps.is_available()); print('Built with MPS:',torch.backends.mps.is_built())"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="run-python-inside-swift" class="level2">
<h2 class="anchored" data-anchor-id="run-python-inside-swift">run python inside swift</h2>
<p>use <a href="https://github.com/pvieito/PythonKit.git">pythonkit</a></p>
</section>
<section id="automatic-machine-learning-using-createml" class="level2">
<h2 class="anchored" data-anchor-id="automatic-machine-learning-using-createml">automatic machine learning using CreateML</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode swift code-with-copy"><code class="sourceCode swift"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="im">CreateML</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>CreateML is similar to any other <a href="https://www.automl.org/automl/">AutoML</a> tools, like <a href="https://autokeras.com/">AutoKeras</a>, <a href="https://huggingface.co/autotrain">AutoTrain by Huggingface</a> (works by training against a selected set of user-provided models)</p>
</section>
<section id="using-coreml" class="level2">
<h2 class="anchored" data-anchor-id="using-coreml">using CoreML</h2>
<p><a href="https://github.com/likedan/Awesome-CoreML-Models">curated, largest coreml models collection</a></p>
<p>CoreML models can be created by CreateML and some customization can be done via <code>protocol MLCustomLayer</code>.</p>
<p><a href="https://onnxruntime.ai/">onnxruntime</a> can run onnx models on CoreML, via c#, since that library is maintained by microsoft.</p>
<p>to install c# on macos:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install dotnet-sdk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>to install and launch <a href="https://github.com/jonsequitur/dotnet-repl">dotnet repl</a>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">dotnet</span> tool install <span class="at">-g</span> dotnet-repl</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">dotnet</span> repl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="paddlepaddle-support" class="level2">
<h2 class="anchored" data-anchor-id="paddlepaddle-support">paddlepaddle support</h2>
<p>convert into onnx first, then run on onnxruntime.</p>
<p>paddlepaddle itself currently only supports running on M1 CPU only via rosetta 2.</p>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<p><a href="https://github.com/huggingface/swift-coreml-transformers">Swift Core ML 3 implementations of GPT-2, DistilGPT-2, BERT, and DistilBERT for Question answering.</a></p>
<p><a href="https://www.appcoda.com/create-ml/">train image classifier and text classifier</a> in which <code>CreateMLUI</code> is deprecated (gone)</p>
<p><a href="https://flight.school/articles/classifying-programming-languages-with-createml/">train source code classifier</a> with <a href="https://flight.school/">flightschool</a> which is a free swift tutorial books provider</p>
<p><a href="https://developer.apple.com/documentation/soundanalysis/classifying_sounds_in_an_audio_file">classifying sounds with coreml</a> return sound type along with timestamp</p>
<p><a href="https://developer.apple.com/documentation/coreml/model_integration_samples/detecting_human_body_poses_in_an_image">detect human pose using coreml</a></p>
<p><a href="https://developer.apple.com/documentation/speech/sfspeechrecognitionrequest">apple speech recognization api request</a></p>
<p><a href="https://pytorch.org/docs/stable/notes/mps.html">pytorch mps backend</a></p>
<p><a href="https://heartbeat.comet.ml/text-classification-on-ios-using-create-ml-f71d7191404a">text classification using createml</a></p>
<p><a href="https://github.com/onnx/models">onnx model zoo</a></p>
<p><a href="https://developer.apple.com/documentation/coreml/getting_a_core_ml_model">Getting CoreML Models</a></p>
<p><a href="https://developer.apple.com/machine-learning/models/">CoreML Model Zoo</a></p>
<details>
<p>
</p><p>FCRN-DepthPrediction</p>
<p>Depth Estimation</p>
<p>Predict the depth from a single image.</p>
<p>View Models</p>
<p>MNIST</p>
<p>Drawing Classification</p>
<p>Classify a single handwritten digit (supports digits 0-9).</p>
<p>View Model</p>
<p>UpdatableDrawingClassifier</p>
<p>Drawing Classification</p>
<p>Drawing classifier that learns to recognize new drawings based on a K-Nearest Neighbors model (KNN).</p>
<p>View Model and Code Sample</p>
<p>MobileNetV2</p>
<p>Image Classification</p>
<p>The MobileNetv2 architecture trained to classify the dominant object in a camera frame or image.</p>
<p>View Models and Code Sample</p>
<p>Resnet50</p>
<p>Image Classification</p>
<p>A Residual Neural Network that will classify the dominant object in a camera frame or image.</p>
<p>View Models and Code Sample</p>
<p>SqueezeNet</p>
<p>Image Classification</p>
<p>A small Deep Neural Network architecture that classifies the dominant object in a camera frame or image.</p>
<p>View Models and Code Sample</p>
<p>DeeplabV3</p>
<p>Image Segmentation</p>
<p>Segment the pixels of a camera frame or image into a predefined set of classes.</p>
<p>View Models</p>
<p>YOLOv3</p>
<p>Object Detection</p>
<p>Locate and classify 80 different types of objects present in a camera frame or image.</p>
<p>View Models and Code Sample</p>
<p>YOLOv3-Tiny</p>
<p>Object Detection</p>
<p>Locate and classify 80 different types of objects present in a camera frame or image.</p>
<p>View Models and Code Sample</p>
<p>PoseNet</p>
<p>Pose Estimation</p>
<p>Estimates up to 17 joint positions for each person in an image.</p>
<p>View Models and Code Sample</p>
<p>Text</p>
<p>BERT-SQuAD</p>
<p>Question Answering</p>
<p>Find answers to questions about paragraphs of text.</p>
<p>View Model and Code Sample</p>
<p></p>
</details>
<p><a href="https://developer.apple.com/machine-learning/api/">Apple Machine Learning Related APIs</a> (may need user permission within or without xcode by means of Info.plist or something)</p>
<details>
<p>
</p><p>Vision</p>
<p>Build features that can process and analyze images and video using computer vision.</p>
<p>View Vision framework</p>
<p>Image Classification</p>
<p>Automatically identify the content in images.</p>
<p>View API</p>
<p>Image Saliency</p>
<p>Quantify and visualize the key part of an image or where in the image people are likely to look.</p>
<p>View API</p>
<p>Image Alignment</p>
<p>Analyze and manage the alignment of images.</p>
<p>View API</p>
<p>Image Similarity</p>
<p>Generate a feature print to compute distance between images.</p>
<p>View API</p>
<p>Object Detection</p>
<p>Find and label objects in images.</p>
<p>View API</p>
<p>Object Tracking</p>
<p>Track moving objects in video.</p>
<p>View API</p>
<p>Trajectory Detection</p>
<p>Detect the trajectory of objects in motion in video.</p>
<p>View API</p>
<p>Contour Detection</p>
<p>Trace the edges of objects and features in images and video.</p>
<p>View API</p>
<p>Text Detection</p>
<p>Detect regions of visible text in images.</p>
<p>View API</p>
<p>Text Recognition</p>
<p>Find, recognize, and extract text from images.</p>
<p>View API</p>
<p>Face Detection</p>
<p>Detect human faces in images.</p>
<p>View API</p>
<p>Face Tracking</p>
<p>Track faces from a camera feed in real time.</p>
<p>View API</p>
<p>Face Landmarks</p>
<p>Find facial features in images by detecting landmarks on faces.</p>
<p>View API</p>
<p>Face Capture Quality</p>
<p>Compare face capture quality in a set of images.</p>
<p>View API</p>
<p>Human Body Detection</p>
<p>Find regions that contain human bodies in images.</p>
<p>View API</p>
<p>Body Pose</p>
<p>Detect landmarks on people in images and video.</p>
<p>View API</p>
<p>Hand Pose</p>
<p>Detect landmarks on human hands in images and video.</p>
<p>View API</p>
<p>Animal Recognition</p>
<p>Find cats and dogs in images.</p>
<p>View API</p>
<p>Barcode Detection</p>
<p>Detect and analyze barcodes in images.</p>
<p>View API</p>
<p>Rectangle Detection</p>
<p>Find rectangular regions in images.</p>
<p>View API</p>
<p>Horizon Detection</p>
<p>Determine the horizon angle in images.</p>
<p>View API</p>
<p>Optical Flow</p>
<p>Analyze the pattern of motion of objects between consecutive video frames.</p>
<p>View API</p>
<p>Person Segmentation New</p>
<p>Produce a matte image for a person in an image.</p>
<p>View API</p>
<p>Document Detection New</p>
<p>Detect rectangular regions in images that contain text.</p>
<p>View API</p>
<p>Natural Language</p>
<p>Analyze natural language text and deduce its language-specific metadata.</p>
<p>View Natural Language framework</p>
<p>Tokenization</p>
<p>Enumerate the words in text strings.</p>
<p>View API</p>
<p>Language Identification</p>
<p>Recognize the language of bodies of text.</p>
<p>View API</p>
<p>Named Entity Recognition</p>
<p>Use a linguistic tagger to name entities in a string.</p>
<p>View API</p>
<p>Part of Speech Tagging</p>
<p>Classify nouns, verbs, adjectives, and other parts of speech in a string.</p>
<p>View API</p>
<p>Word Embedding</p>
<p>Get a vector representation for any word and find similarity between two words or nearest neighbors for a word.</p>
<p>View API</p>
<p>Sentence Embedding</p>
<p>Get a vector representation for any string and find similarity between two strings.</p>
<p>View API</p>
<p>Sentiment Analysis</p>
<p>Score text as positive, negative, or neutral based on the sentiment.</p>
<p>View API</p>
<p>Speech</p>
<p>Take advantage of speech recognition and saliency features for a variety of languages.</p>
<p>View Speech framework</p>
<p>Speech Recognition</p>
<p>Recognize and analyze speech in audio and get back data like transcripts.</p>
<p>View API</p>
<p>Sound Analysis</p>
<p>Analyze audio and recognize it as a particular type, such as laughter or applause.</p>
<p>View Sound Analysis framework</p>
<p>Sound Classification</p>
<p>Analyze sounds in audio using the built-in sound classifier or a custom Core ML sound classification model.</p>
<p>View API</p>
<p></p>
</details>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>